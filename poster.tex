\documentclass[final]{beamer}
\usetheme{RJH}
\usepackage[orientation=landscape,size=a0,scale=1.4,debug]{beamerposter}
\usepackage[absolute,overlay]{textpos}
\setlength{\TPHorizModule}{1cm}
\setlength{\TPVertModule}{1cm}

\title{Fast Point Cloud Registration using Gaussian Processes}
\author{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~Ben Eckart, Seth Flaxman, Antonio Juarez}
\footer{10-725 final project}
\date{}

\begin{document}
\bibliographystyle{plain}
\begin{frame}{} 

\begin{textblock}{27}(1.5,5)
\begin{block}{Motivation: Automatic Mapping of the World}
\begin{figure}
\includegraphics[width=10in]{kyle_kinect.jpg}
\caption{Photo (CC-BY-SA-NC) Kyle McDonald}
\end{figure}

3D range sensors like Velodyne and Kinect generate massive amounts of data, on the order of
{\bf one million data points per second}.

Standard optimization algorithms for real-time perception aren't adequate.

\end{block}

\begin{block}{Related Work: The Registration Problem}

{\bf Iterative Closest Point (ICP) algorithm} 
\begin{figure}
\includegraphics[width=10in]{icpCpp.jpg}
\caption{Photo from Per Bergström via MATLAB FileExchange}
\end{figure}

{\bf Gaussian Mixture Models (GMM)}
\begin{figure}
\includegraphics[width=10in]{register2D.png}
\end{figure}

\end{block}
\end{textblock}

\begin{textblock}{27.5}(30.8,5)
\begin{block}{Gaussian Processes (GPs)}
A distribution over functions, fully specified by a mean and a covariance:
$f \sim \mathcal{GP}(m,k)$. Given a set of points $(X,y)$ and a new point $X_*$ for which we want to predict $y_*$, 
distribution is simple: {\bf multivariate Gaussian} with:
$$\mu = K_* (K + \sigma^2 I)^{-1}y$$
$$\Sigma = K_{**} - K_* (K + \sigma^2 I)^{-1} K_*^T$$
\end{block}

\begin{block}{Formulating Registration as an Optimization Problem}
Given a scene and a set of new points, find a rigid transformation $T = [T_x,T_y,T_z,T_s,T_u,T_v,T_w]$ of the new points 
that maximizes their likelihood in the original scene, where the original scene is fit with a GP:
$$min -\ell(T|X,y,X_*,y_*) =$$
$$ \ln|\Sigma| + (T(X_*) - \mu)^T \Sigma^{-1} (T(X_*) - \mu)$$

Note that this problem is not convex---in fact, the minimum we care about may {\bf not} be the global minimum. 
\end{block}
\begin{block}{Gradient Descent}
Partial derivatives must be found with respect to the 7 parameters of $T$. Each can be written in closed form,
using {\bf matrix} and {\bf quaternion} calculus. Numerical stability and speed relies on the {\bf Cholesky} decomposition.
\end{block}

\end{textblock}

\begin{textblock}{27}(61,5)

\begin{block}{Experimental Evaluation}
\begin{figure}
\includegraphics[width=10in]{likelihood.png}
\caption{Convergence}
\end{figure}

\end{block}

\begin{block}{References}
{\scriptsize
 \bibliographystyle{abbrv}
 \bibliography{cmubib}
}
\end{block}
\end{textblock}


\begin{textblock}{27}(91,5)


\end{textblock}

\end{frame}
\end{document}
